#!/usr/bin/env python3
"""
Vector-Based Knowledge Graph Demo

Demonstrates the corrected implementation that:
1. Ingests actual vector embeddings from ChromaDB
2. Extracts semantic concepts from content chunks
3. Creates relationships based on vector similarity
4. Builds proper knowledge graphs from real data
"""

import requests
import json
from pathlib import Path

def demonstrate_vector_ingestion():
    """Demonstrate how the new system ingests vector data instead of simple document nodes."""
    print("ğŸ”¬ VECTOR DATA INGESTION DEMONSTRATION")
    print("=" * 50)

    print("ğŸ“Š Old Approach (Fixed):")
    print("âŒ Created simple document nodes: doc_0, doc_1, etc.")
    print("âŒ Only used document metadata and file types")
    print("âŒ No semantic understanding of content")

    print("\nğŸ“Š New Approach (Current):")
    print("âœ… Extracts vector embeddings from ChromaDB chunks")
    print("âœ… Analyzes semantic content of each chunk")
    print("âœ… Creates concept entities based on frequency analysis")
    print("âœ… Groups content using vector similarity clustering")
    print("âœ… Builds relationships based on co-occurrence and similarity")

def test_vector_knowledge_graph_features():
    """Test the vector-based knowledge graph features."""
    print("\nğŸ§ª TESTING VECTOR KNOWLEDGE GRAPH FEATURES")
    print("=" * 50)

    base_url = "http://localhost:9090"

    try:
        # Test the vector chunks endpoint
        print("Testing Vector Chunks Extraction...")
        response = requests.get(f"{base_url}/api/visualizations/knowledge-graph")

        if response.status_code == 200:
            data = response.json()
            metadata = data.get('metadata', {})

            print(f"   ğŸ“Š Data Source: {metadata.get('data_source', 'Unknown')}")
            print(f"   ğŸ”¬ Extraction Method: {metadata.get('extraction_method', 'Unknown')}")

            if 'chunk_count' in metadata:
                print(f"   ğŸ“„ Vector Chunks: {metadata.get('chunk_count', 0)}")

            if 'concept_count' in metadata:
                print(f"   ğŸ’¡ Semantic Concepts: {metadata.get('concept_count', 0)}")

            if 'cluster_count' in metadata:
                print(f"   ğŸ”— Content Clusters: {metadata.get('cluster_count', 0)}")

            if metadata.get('total_entities', 0) == 0:
                print("   â„¹ï¸  No documents uploaded yet - upload documents to see vector analysis")
            else:
                print(f"   âœ… Generated {metadata.get('total_entities', 0)} entities from vector data")

        else:
            print(f"   âŒ API Error: {response.status_code}")

    except Exception as e:
        print(f"   âŒ Test Error: {e}")

def explain_vector_processing_pipeline():
    """Explain how the vector knowledge graph processing works."""
    print("\nğŸ”„ VECTOR PROCESSING PIPELINE")
    print("=" * 50)

    print("1. ğŸ“¥ Vector Data Extraction:")
    print("   â€¢ Retrieves document chunks with embeddings from ChromaDB")
    print("   â€¢ Includes: text content, metadata, vector embeddings, source info")

    print("\n2. ğŸ§  Semantic Concept Extraction:")
    print("   â€¢ Analyzes text content using frequency analysis")
    print("   â€¢ Extracts: Capitalized phrases, technical terms, acronyms")
    print("   â€¢ Filters by frequency and relevance")

    print("\n3. ğŸ”— Vector Similarity Clustering:")
    print("   â€¢ Groups content chunks using K-means clustering on embeddings")
    print("   â€¢ Creates content cluster entities representing similar topics")
    print("   â€¢ Generates descriptive names from cluster content")

    print("\n4. ğŸ“š Source Entity Creation:")
    print("   â€¢ Creates entities for each document source")
    print("   â€¢ Tracks chunk count and file metadata")

    print("\n5. ğŸ•¸ï¸ Relationship Building:")
    print("   â€¢ Concept â†” Source: Based on text co-occurrence")
    print("   â€¢ Cluster â†” Source: Based on vector similarity groupings")
    print("   â€¢ Concept â†” Concept: Based on co-occurrence patterns")

def show_knowledge_graph_capabilities():
    """Show what the vector-based knowledge graph can reveal."""
    print("\nğŸ¯ KNOWLEDGE GRAPH CAPABILITIES")
    print("=" * 50)

    print("ğŸ“ˆ What the Vector Knowledge Graph Reveals:")

    print("\nğŸ” Semantic Analysis:")
    print("   â€¢ Key concepts and terms across your documents")
    print("   â€¢ Frequency and importance of topics")
    print("   â€¢ Technical terms, acronyms, and proper nouns")

    print("\nğŸ”— Content Relationships:")
    print("   â€¢ Documents with similar semantic content")
    print("   â€¢ Topics that frequently appear together")
    print("   â€¢ Conceptual clusters in your knowledge base")

    print("\nğŸ“Š Vector Insights:")
    print("   â€¢ Content similarity based on embeddings")
    print("   â€¢ Semantic groupings of related chunks")
    print("   â€¢ Knowledge distribution across sources")

    print("\nğŸ¨ Interactive Visualization:")
    print("   â€¢ D3.js network graph with drag/zoom")
    print("   â€¢ Color-coded entity types (concepts, clusters, sources)")
    print("   â€¢ Relationship strength shown by line thickness")
    print("   â€¢ Real-time updates when documents are added/removed")

def demonstrate_data_flow():
    """Demonstrate the data flow from upload to visualization."""
    print("\nğŸ“‹ DATA FLOW: UPLOAD TO VISUALIZATION")
    print("=" * 50)

    print("1. ğŸ“¤ User uploads document via Knowledge Base")
    print("   â†“")
    print("2. ğŸ”§ RAG system processes document into chunks")
    print("   â†“")
    print("3. ğŸ§® Sentence transformer creates embeddings")
    print("   â†“")
    print("4. ğŸ’¾ ChromaDB stores: text + metadata + embeddings")
    print("   â†“")
    print("5. ğŸ”¬ Knowledge graph builder analyzes vector data")
    print("   â†“")
    print("6. ğŸ¨ Visualization shows semantic relationships")

    print("\nğŸ”„ Real-time Updates:")
    print("   â€¢ Add document â†’ New concepts and clusters appear")
    print("   â€¢ Remove document â†’ Related entities disappear")
    print("   â€¢ All relationships update automatically")
    print("   â€¢ Vector similarity recalculated on demand")

def main():
    """Run the complete vector knowledge graph demonstration."""
    print("ğŸš€ VECTOR-BASED KNOWLEDGE GRAPH DEMONSTRATION")
    print("=" * 60)

    # Demonstrate the improvement
    demonstrate_vector_ingestion()

    # Test current features
    test_vector_knowledge_graph_features()

    # Explain the processing pipeline
    explain_vector_processing_pipeline()

    # Show capabilities
    show_knowledge_graph_capabilities()

    # Demonstrate data flow
    demonstrate_data_flow()

    print("\n" + "=" * 60)
    print("âœ… VECTOR KNOWLEDGE GRAPH IMPLEMENTATION COMPLETE")
    print("=" * 60)

    print("\nğŸ¯ Key Improvements Made:")
    print("1. âœ… Now ingests actual vector embeddings from ChromaDB")
    print("2. âœ… Extracts semantic concepts from content analysis")
    print("3. âœ… Creates relationships based on vector similarity")
    print("4. âœ… Builds proper knowledge graphs from chunk data")

    print("\nğŸ“– How to See Vector Analysis:")
    print("1. Open: http://localhost:9090")
    print("2. Upload documents via Knowledge section")
    print("3. Go to Visualizations â†’ Knowledge Graph")
    print("4. See semantic concepts and vector-based clusters")

    print("\nğŸ”¬ Technical Details:")
    print("â€¢ Vector embeddings: Sentence transformers (all-MiniLM-L6-v2)")
    print("â€¢ Clustering: K-means on embedding vectors")
    print("â€¢ Concept extraction: Frequency analysis + NLP patterns")
    print("â€¢ Relationships: Co-occurrence + vector similarity")
    print("â€¢ Real-time: Updates with every document change")

if __name__ == "__main__":
    main()