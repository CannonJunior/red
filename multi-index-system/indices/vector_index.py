"""
Vector Index Implementation using ChromaDB

Provides semantic search capabilities using embeddings generated by
local Ollama models. Supports multiple collections and workspaces.
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Set
from datetime import datetime
import hashlib
import json

try:
    from ..config.settings import get_config
    from ..config.ollama_config import OllamaConfig
    from .base import IndexInterface, IndexCapabilities, QueryResult, IndexStats
except ImportError:
    from config.settings import get_config
    from config.ollama_config import OllamaConfig
    from indices.base import IndexInterface, IndexCapabilities, QueryResult, IndexStats

try:
    import chromadb
    from chromadb.config import Settings
except ImportError:
    chromadb = None

logger = logging.getLogger(__name__)

class VectorIndex(IndexInterface):
    """
    ChromaDB-based vector index for semantic search.

    Features:
    - Automatic embedding generation using local Ollama
    - Multiple collections per workspace
    - Similarity search with confidence scoring
    - Metadata filtering and hybrid queries
    """

    def __init__(self, index_name: str, data_path: str, config: Dict[str, Any]):
        super().__init__(index_name, data_path, config)

        self.client = None
        self.collections = {}  # workspace -> collection mapping
        self.embedding_model = config.get('embedding_model', 'nomic-embed-text')
        self.embedding_dimension = config.get('embedding_dimension', 768)
        self.max_results = config.get('max_results', 100)

        # Initialize Ollama for embeddings
        self.ollama_config = OllamaConfig()

    async def initialize(self) -> bool:
        """Initialize ChromaDB client and verify connection."""
        try:
            if chromadb is None:
                self.logger.error("ChromaDB not installed. Run: uv add chromadb")
                return False

            # Create persistent ChromaDB client
            chroma_settings = Settings(
                persist_directory=str(self.data_path / "chroma"),
                anonymized_telemetry=False
            )

            self.client = chromadb.PersistentClient(
                path=str(self.data_path / "chroma"),
                settings=chroma_settings
            )

            # Test connection
            self.client.heartbeat()
            self.logger.info(f"ChromaDB initialized at {self.data_path / 'chroma'}")

            # Verify embedding model availability
            if not await self._verify_embedding_model():
                self.logger.warning(f"Embedding model {self.embedding_model} not available")
                return False

            return True

        except Exception as e:
            self.logger.error(f"Failed to initialize ChromaDB: {e}")
            return False

    async def shutdown(self):
        """Gracefully shutdown the vector index."""
        try:
            if self.client:
                self.client = None
                self.collections.clear()
                self.logger.info("Vector index shutdown complete")
        except Exception as e:
            self.logger.error(f"Error during vector index shutdown: {e}")

    async def insert(self, documents: List[Dict[str, Any]], workspace: str = "default") -> Dict[str, Any]:
        """Insert documents with automatic embedding generation."""
        if not self.client:
            raise RuntimeError("Vector index not initialized")

        workspace = self._validate_workspace(workspace)
        prepared_docs = self._prepare_documents(documents)

        if not prepared_docs:
            return {"status": "error", "message": "No valid documents to insert"}

        start_time = datetime.now()

        try:
            # Get or create collection for workspace
            collection = await self._get_collection(workspace)

            # Generate embeddings for documents
            embeddings_data = await self._generate_embeddings(prepared_docs)

            # Prepare data for ChromaDB
            ids = [doc['id'] for doc in prepared_docs]
            embeddings = embeddings_data['embeddings']
            metadatas = [self._extract_metadata(doc) for doc in prepared_docs]
            documents_text = [self._extract_searchable_text(doc) for doc in prepared_docs]

            # Insert into ChromaDB
            collection.add(
                ids=ids,
                embeddings=embeddings,
                metadatas=metadatas,
                documents=documents_text
            )

            execution_time = (datetime.now() - start_time).total_seconds()
            self._track_query_performance(execution_time)

            self.logger.info(f"Inserted {len(prepared_docs)} documents into vector index")

            return {
                "status": "success",
                "documents_inserted": len(prepared_docs),
                "execution_time": execution_time,
                "workspace": workspace
            }

        except Exception as e:
            self.logger.error(f"Vector insert failed: {e}")
            return {"status": "error", "message": str(e)}

    async def update(self, document_updates: List[Dict[str, Any]], workspace: str = "default") -> Dict[str, Any]:
        """Update existing documents in the vector index."""
        if not self.client:
            raise RuntimeError("Vector index not initialized")

        workspace = self._validate_workspace(workspace)
        start_time = datetime.now()

        try:
            collection = await self._get_collection(workspace)
            updated_count = 0

            for update in document_updates:
                if 'id' not in update:
                    self.logger.warning("Skipping update without document ID")
                    continue

                doc_id = update['id']

                # Check if document exists
                try:
                    existing = collection.get(ids=[doc_id])
                    if not existing['ids']:
                        self.logger.warning(f"Document {doc_id} not found for update")
                        continue
                except Exception:
                    continue

                # Generate new embedding if content changed
                if any(key in update for key in ['content', 'text', 'title']):
                    embedding_data = await self._generate_embeddings([update])
                    embedding = embedding_data['embeddings'][0]
                else:
                    embedding = None

                # Update document
                collection.update(
                    ids=[doc_id],
                    embeddings=[embedding] if embedding else None,
                    metadatas=[self._extract_metadata(update)],
                    documents=[self._extract_searchable_text(update)]
                )

                updated_count += 1

            execution_time = (datetime.now() - start_time).total_seconds()
            self._track_query_performance(execution_time)

            return {
                "status": "success",
                "documents_updated": updated_count,
                "execution_time": execution_time
            }

        except Exception as e:
            self.logger.error(f"Vector update failed: {e}")
            return {"status": "error", "message": str(e)}

    async def delete(self, document_ids: List[str], workspace: str = "default") -> Dict[str, Any]:
        """Delete documents from the vector index."""
        if not self.client:
            raise RuntimeError("Vector index not initialized")

        workspace = self._validate_workspace(workspace)
        start_time = datetime.now()

        try:
            collection = await self._get_collection(workspace)

            # Filter existing IDs
            existing = collection.get(ids=document_ids)
            existing_ids = existing['ids']

            if existing_ids:
                collection.delete(ids=existing_ids)

            execution_time = (datetime.now() - start_time).total_seconds()
            self._track_query_performance(execution_time)

            return {
                "status": "success",
                "documents_deleted": len(existing_ids),
                "execution_time": execution_time
            }

        except Exception as e:
            self.logger.error(f"Vector delete failed: {e}")
            return {"status": "error", "message": str(e)}

    async def query(self, query_params: Dict[str, Any], workspace: str = "default") -> QueryResult:
        """Execute semantic search query."""
        if not self.client:
            raise RuntimeError("Vector index not initialized")

        workspace = self._validate_workspace(workspace)
        start_time = datetime.now()

        try:
            collection = await self._get_collection(workspace)

            # Extract query parameters
            query_text = query_params.get('query', query_params.get('text', ''))
            n_results = query_params.get('limit', self.max_results)
            where_filter = query_params.get('where', {})

            if not query_text:
                raise ValueError("Query text required for vector search")

            # Generate query embedding
            query_embedding_data = await self._generate_embeddings([{'text': query_text}])
            query_embedding = query_embedding_data['embeddings'][0]

            # Execute similarity search
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_filter if where_filter else None
            )

            # Format results
            documents = []
            confidence_scores = []

            for i, doc_id in enumerate(results['ids'][0]):
                doc = {
                    'id': doc_id,
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i] or {},
                    'distance': results['distances'][0][i]
                }

                # Convert distance to confidence score (0-1)
                confidence = max(0, 1 - results['distances'][0][i])
                confidence_scores.append(confidence)

                documents.append(doc)

            execution_time = (datetime.now() - start_time).total_seconds()
            self._track_query_performance(execution_time)

            return QueryResult(
                documents=documents,
                metadata={
                    "query_embedding_model": self.embedding_model,
                    "similarity_metric": "cosine",
                    "workspace": workspace
                },
                total_found=len(documents),
                execution_time=execution_time,
                index_used=self.index_name,
                confidence_scores=confidence_scores
            )

        except Exception as e:
            self.logger.error(f"Vector query failed: {e}")
            raise

    async def health_check(self) -> Dict[str, Any]:
        """Check vector index health and connectivity."""
        health_data = {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "checks": {}
        }

        try:
            # Check ChromaDB connection
            if self.client:
                self.client.heartbeat()
                health_data["checks"]["chromadb_connection"] = "healthy"
            else:
                health_data["checks"]["chromadb_connection"] = "disconnected"
                return health_data

            # Check embedding model
            model_available = await self._verify_embedding_model()
            health_data["checks"]["embedding_model"] = "healthy" if model_available else "unavailable"

            # Check collections
            collections_count = len(self.collections)
            health_data["checks"]["collections"] = f"{collections_count} active"

            # Overall status
            if all(check in ["healthy", f"{collections_count} active"]
                   for check in health_data["checks"].values()):
                health_data["status"] = "healthy"

        except Exception as e:
            health_data["checks"]["error"] = str(e)

        return health_data

    def get_capabilities(self) -> Set[IndexCapabilities]:
        """Return supported capabilities."""
        return {
            IndexCapabilities.SEMANTIC_SEARCH,
            IndexCapabilities.FUZZY_MATCHING,
            IndexCapabilities.AGGREGATION
        }

    async def optimize(self) -> Dict[str, Any]:
        """Optimize vector index performance."""
        try:
            optimization_results = {
                "status": "completed",
                "timestamp": datetime.now().isoformat(),
                "optimizations": []
            }

            # ChromaDB doesn't require explicit optimization
            # but we can provide collection statistics
            for workspace, collection in self.collections.items():
                try:
                    count = collection.count()
                    optimization_results["optimizations"].append({
                        "workspace": workspace,
                        "document_count": count,
                        "action": "verified"
                    })
                except Exception as e:
                    optimization_results["optimizations"].append({
                        "workspace": workspace,
                        "error": str(e)
                    })

            return optimization_results

        except Exception as e:
            return {"status": "failed", "error": str(e)}

    async def get_stats(self) -> IndexStats:
        """Get comprehensive vector index statistics."""
        try:
            total_documents = 0
            storage_size = 0

            # Calculate total documents across all collections
            for collection in self.collections.values():
                try:
                    total_documents += collection.count()
                except Exception:
                    pass

            # Estimate storage size (ChromaDB doesn't provide direct size info)
            chroma_path = self.data_path / "chroma"
            if chroma_path.exists():
                storage_size = sum(f.stat().st_size for f in chroma_path.rglob('*') if f.is_file())

            return IndexStats(
                document_count=total_documents,
                storage_size_bytes=storage_size,
                avg_query_time=self.get_avg_query_time(),
                total_queries=self.query_count,
                last_updated=self.last_query_time or datetime.now(),
                health_status="healthy" if self.client else "disconnected",
                capabilities=self.get_capabilities()
            )

        except Exception as e:
            self.logger.error(f"Failed to get vector index stats: {e}")
            return IndexStats(
                document_count=0,
                storage_size_bytes=0,
                avg_query_time=0.0,
                total_queries=0,
                last_updated=datetime.now(),
                health_status="error",
                capabilities=set()
            )

    # Helper methods

    async def _get_collection(self, workspace: str):
        """Get or create ChromaDB collection for workspace."""
        if workspace not in self.collections:
            collection_name = f"{self.index_name}_{workspace}"

            try:
                # Try to get existing collection
                collection = self.client.get_collection(collection_name)
            except Exception:
                # Create new collection
                collection = self.client.create_collection(
                    name=collection_name,
                    metadata={"workspace": workspace, "index_type": "vector"}
                )

            self.collections[workspace] = collection

        return self.collections[workspace]

    async def _generate_embeddings(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate embeddings for documents using Ollama."""
        try:
            texts = [self._extract_searchable_text(doc) for doc in documents]
            embeddings = []

            for text in texts:
                # Use Ollama to generate embedding
                response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.ollama_config.generate_embedding(self.embedding_model, text)
                )

                if response and 'embedding' in response:
                    embeddings.append(response['embedding'])
                else:
                    # Fallback: create zero embedding
                    embeddings.append([0.0] * self.embedding_dimension)
                    self.logger.warning(f"Failed to generate embedding for text: {text[:50]}...")

            return {
                "embeddings": embeddings,
                "model": self.embedding_model,
                "dimension": len(embeddings[0]) if embeddings else 0
            }

        except Exception as e:
            self.logger.error(f"Embedding generation failed: {e}")
            # Return zero embeddings as fallback
            return {
                "embeddings": [[0.0] * self.embedding_dimension] * len(documents),
                "model": "fallback",
                "dimension": self.embedding_dimension
            }

    async def _verify_embedding_model(self) -> bool:
        """Verify that the embedding model is available."""
        try:
            test_response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.ollama_config.generate_embedding(self.embedding_model, "test")
            )
            return test_response is not None and 'embedding' in test_response
        except Exception:
            return False

    def _extract_searchable_text(self, document: Dict[str, Any]) -> str:
        """Extract searchable text from document."""
        text_fields = ['content', 'text', 'title', 'description', 'summary']

        parts = []
        for field in text_fields:
            if field in document and document[field]:
                parts.append(str(document[field]))

        return ' '.join(parts) if parts else document.get('id', '')

    def _extract_metadata(self, document: Dict[str, Any]) -> Dict[str, Any]:
        """Extract metadata for ChromaDB storage."""
        metadata = {}

        # Copy safe metadata fields (ChromaDB has restrictions on metadata types)
        safe_fields = ['title', 'author', 'category', 'source', 'type', 'tags']

        for field in safe_fields:
            if field in document:
                value = document[field]
                # Convert lists to strings for ChromaDB compatibility
                if isinstance(value, list):
                    metadata[field] = ','.join(str(v) for v in value)
                elif isinstance(value, (str, int, float, bool)):
                    metadata[field] = value

        # Add indexing metadata
        metadata['_indexed_at'] = document.get('_indexed_at', datetime.now().isoformat())

        return metadata